{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPk87xJFfEDxNNCIpGoX7hN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mosesds/CS7650-project/blob/master/CS7650_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaeTo2DDqE16",
        "outputId": "2d63b810-0b98-4465-a6fd-91d650f1600d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 11 22:05:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    41W / 350W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "GPU available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "print(f'GPU available: {torch.cuda.is_available()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "j9iyiY3t0_sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: DATA PREPROCESSING"
      ],
      "metadata": {
        "id": "V8ggYaUa0WCt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6sU8YFAd1G6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import collections\n",
        "random.seed(0)\n",
        "\n",
        "\n",
        "def prepare_sequence(lst, idx_mapping):\n",
        "    \"\"\" \n",
        "    Map individual items from `sent` using `idx_mapping`\n",
        "    Return value is the same length as `sent`\n",
        "    Usage: \n",
        "        >> prepare_sequence(['a', 'b', 'c'], {'a':0, 'b':1, 'c':2})\n",
        "        [0, 1, 2]\n",
        "    \"\"\"\n",
        "    idxs = []\n",
        "    for item in lst:\n",
        "        if item not in idx_mapping:\n",
        "            assert '<unk>' in idx_mapping or 0 in idx_mapping, \"cannot map unknown token:\" + item\n",
        "            if '<unk>' in idx_mapping:\n",
        "                idxs.append(idx_mapping['<unk>'])\n",
        "            else:\n",
        "                idxs.append(idx_mapping[0])\n",
        "        else:\n",
        "            idxs.append(idx_mapping[item])\n",
        "    try: \n",
        "        return torch.tensor(idxs, dtype=torch.long)\n",
        "    except: \n",
        "        return idxs\n",
        "\n",
        "\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim,\n",
        "                 char_hidden_dim, char_size, vocab_size, lstm_layers=1, \n",
        "                 bidirectional=False, dropout=0):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        #############################################################################\n",
        "        # embedding layer: that maps words to the embedding space\n",
        "        # char embedding layer: maps chars to embedding space \n",
        "        # an char level LSTM: that finds the character level embedding for a word\n",
        "        # an LSTM layer: that takes the combined embeddings as input and outputs hidden states\n",
        "        # Remember, this needs to be done for both context and query (our input)\n",
        "        # (DO NOT apply bidirectionality to character LSTMs)\n",
        "        #############################################################################\n",
        "\n",
        "        self.context_embeddingLayer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.context_charEmbeddingLayer = nn.Embedding(char_size, char_embedding_dim)\n",
        "        self.context_char_LSTM = nn.LSTM(char_embedding_dim, char_hidden_dim,lstm_layers)\n",
        "        self.context_LSTMLayer = nn.LSTM(char_hidden_dim + embedding_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n",
        "        \n",
        "        self.query_embeddingLayer = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.query_charEmbeddingLayer = nn.Embedding(char_size, char_embedding_dim)\n",
        "        self.query_char_LSTM = nn.LSTM(char_embedding_dim, char_hidden_dim,lstm_layers)\n",
        "        self.query_LSTMLayer = nn.LSTM(char_hidden_dim + embedding_dim, hidden_dim, lstm_layers, bidirectional=bidirectional)\n",
        "\n",
        "        # Remember: bidirectional makes the output hidden_dim * 2\n",
        "\n",
        "    def forward(self, context, context_chars, query, query_chars):\n",
        "        lstm_context_vectors = None # for each word\n",
        "        lstm_query_vectors = None # for each word \n",
        "        #############################################################################\n",
        "        # Given a tokenized index-mapped sentence and a character sequence as the arguments,\n",
        "        # `context` and `query` are word sequences at are index mapped\n",
        "        # `context_chars` and `query_chars` are char sequence of each word that are \n",
        "        #   index mapped\n",
        "        # Return values:\n",
        "        #   `lstm_query_vectors` : Txd or Nx(d*2) if bidirectional LSTM used\n",
        "        #       T is # of words in context, and d is size of hidden states\n",
        "        #   `lstm_query_vectors` : Jxd or Jx(d*2) if bidirectional LSTM used\n",
        "        #       J is # of words in query, and d is size of hidden states\n",
        "        # \n",
        "        #############################################################################\n",
        "\n",
        "        # context embedding\n",
        "        context_embeddings = self.context_embeddingLayer(context)\n",
        "        context_char_embeddings = self.context_charEmbeddingLayer(context_chars)\n",
        "\n",
        "        z = int(len(context_chars)/len(context))\n",
        "        context_char_embeddings = context_char_embeddings.view(len(context),z,-1)\n",
        "        context_last_char_embeddings = None\n",
        "        for i in range(0,len(context)):\n",
        "            hidden_state = None\n",
        "            char_hiddenLayer = None\n",
        "            for j in range(0,z):\n",
        "                char_hiddenLayer, hidden_state = self.context_char_LSTM(context_char_embeddings[i][j].view(1,1,-1),hidden_state)\n",
        "            context_last_char_embeddings = char_hiddenLayer if (context_last_char_embeddings == None) else torch.cat((context_last_char_embeddings,char_hiddenLayer),0)\n",
        "\n",
        "        context_cat_layer = torch.cat((context_embeddings.view(len(context),-1), context_last_char_embeddings.view(len(context),-1)),1)\n",
        "        hidden_layer,_ = self.context_LSTMLayer(context_cat_layer.view(len(context),1,-1))\n",
        "        lstm_context_vectors = hidden_layer.view(len(context),-1)\n",
        "\n",
        "        #query embedding\n",
        "        query_embeddings = self.query_embeddingLayer(query)\n",
        "        query_char_embeddings = self.query_charEmbeddingLayer(query_chars)\n",
        "\n",
        "        z = int(len(query_chars)/len(query))\n",
        "        query_char_embeddings = query_char_embeddings.view(len(query),z,-1)\n",
        "        query_last_char_embeddings = None\n",
        "        for i in range(0,len(query)):\n",
        "            hidden_state = None\n",
        "            char_hiddenLayer = None\n",
        "            for j in range(0,z):\n",
        "                char_hiddenLayer, hidden_state = self.context_char_LSTM(query_char_embeddings[i][j].view(1,1,-1),hidden_state)\n",
        "            query_last_char_embeddings = char_hiddenLayer if (query_last_char_embeddings == None) else torch.cat((query_last_char_embeddings,char_hiddenLayer),0)\n",
        "\n",
        "        query_cat_layer = torch.cat((query_embeddings.view(len(query),-1), query_last_char_embeddings.view(len(query),-1)),1)\n",
        "        hidden_layer,_ = self.context_LSTMLayer(query_cat_layer.view(len(query),1,-1))\n",
        "        lstm_query_vectors = hidden_layer.view(len(query),-1)\n",
        "\n",
        "        return lstm_context_vectors, lstm_query_vectors\n",
        "        \n",
        "\n",
        "class AttentionFlow(nn.Module):\n",
        "    def __init__(self, w_dim):\n",
        "        \"\"\"\n",
        "        w_dim : is the same as 6d in the paper. Should be 6*hidden_dim if bidirectionality is True\n",
        "        \"\"\"\n",
        "        super(AttentionFlow, self).__init__()\n",
        "        #############################################################################\n",
        "        # need a linear layer to compute the similarity matrix (no bias according to the paper)\n",
        "        #############################################################################\n",
        "        self.wt = nn.Linear(w_dim,1,bias=False)\n",
        "        self.softmax_function = nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, context, query):\n",
        "        G = None\n",
        "        #############################################################################\n",
        "        # T : number of tokens in context\n",
        "        # J : number of tokens in query\n",
        "        # d : hidden_dimensions (context and query will have d*2 hidden dimensions if using bidirectional LSTM)\n",
        "        # Parameters (from the encoder):\n",
        "        #     context: of size Txd or (d*2) if using bidirectional LSTM\n",
        "        #     query: of size Jxd or (d*2) if using bidirectional LSTM\n",
        "        # For this part, you need to compute a similarity matrix, S\n",
        "        #     S : TxJ\n",
        "        # then use S to build context2query and query2context attention\n",
        "        #     context2query : T x (d*2)\n",
        "        #     query2context : T x (d*2) \n",
        "        #         hint: query2context will be (1, d*2) but it will need to be repeated \n",
        "        #               T times so the dimension is T x (d*2)\n",
        "        # Return :: G which is the query aware context vectors of size (T x d*8)\n",
        "        #     G is obtained by combining `context`, `context2query` and `query2context` \n",
        "        #       as defined in the paper.\n",
        "        #############################################################################\n",
        "        S = None\n",
        "        for i in range(0,len(context)):\n",
        "            context_sim_vec = None\n",
        "            h = context[i]\n",
        "            for j in range(0,len(query)):\n",
        "                u = query[j]\n",
        "                vector = torch.cat((h,u,torch.mul(h,u)))\n",
        "                sim = self.wt(vector)\n",
        "                context_sim_vec = torch.cat((context_sim_vec, sim),0) if context_sim_vec != None else sim\n",
        "            context_sim_vec = context_sim_vec.view(1,len(query))\n",
        "            S = context_sim_vec if S == None else torch.cat((S,context_sim_vec),0)\n",
        "\n",
        "        context2query = None\n",
        "        query2context = None\n",
        "        for t in range(0,len(context)):\n",
        "            a_t = self.softmax_function(S[t])\n",
        "            ut = None\n",
        "            for j in range(0,len(query)):\n",
        "                ut = a_t[j] * query[j] if ut == None else ut + (a_t[j] * query[j])\n",
        "            ut = ut.view(1,-1)\n",
        "            context2query = ut if context2query == None else torch.cat((context2query, ut), 0)\n",
        "\n",
        "        #context2query = torch.transpose(context2query, 0, 1)\n",
        "        #S_inverse = torch.transpose(S, 0, 1)\n",
        "        b = torch.argmax(S,dim=1)\n",
        "        b = self.softmax_function(b.float())\n",
        "        h_tilda = None\n",
        "\n",
        "        for i in range(0,len(context)):\n",
        "            b_t = b[i]\n",
        "            h_t = context[t]\n",
        "            h_tilda = b_t * h_t if h_tilda == None else (b_t * h_t) + h_tilda\n",
        "        big_h_tilda = h_tilda.repeat(len(context)).view(len(context),-1)\n",
        "        query2context = torch.transpose(big_h_tilda, 0, 1)\n",
        "        for t in range(0,len(context)):\n",
        "            h = context[t]\n",
        "            h_tilda = h_tilda\n",
        "            u_tilda = context2query[t]\n",
        "            vector = torch.cat((h, u_tilda, torch.mul(h,u_tilda), torch.mul(h,h_tilda)),0)\n",
        "            vector = vector.view(1,-1)\n",
        "            G = vector if G == None else torch.cat((G,vector),0)\n",
        "\n",
        "        return G\n",
        "\n",
        "\n",
        "class ModelingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_layers=2, dropout=0.2, \n",
        "        bidirectional=True):\n",
        "        super(ModelingLayer, self).__init__()\n",
        "        #############################################################################\n",
        "        # just need to pass our query aware context vectors \n",
        "        # initialize an LSTM layer here\n",
        "        #############################################################################\n",
        "        self.lstmLayer = nn.LSTM(input_dim, output_dim, num_layers, dropout=dropout,bidirectional=bidirectional)\n",
        "    \n",
        "    def forward(self, G):\n",
        "        M = None\n",
        "        #############################################################################\n",
        "        # G : query aware context word embeddings \n",
        "        # returns :: of size Tx(output_dim*2) (T is # words in context)\n",
        "        #############################################################################\n",
        "        M,_ = self.lstmLayer(G.view(len(G),1,-1))\n",
        "        M = M.view(len(G),-1)\n",
        "\n",
        "        return M\n",
        "\n",
        "\n",
        "class OutputLayer(nn.Module):\n",
        "    def __init__(self, fc_dim, LSTM_input_size, LSTM_output_size, num_layers=1, bidirectional=True):\n",
        "        super(OutputLayer, self).__init__()\n",
        "        #############################################################################\n",
        "        # For the OutputLayer, we need:\n",
        "        #   Linear layer (to predict start idx; no bias for these linear layers according to paper) \n",
        "        #   LSTM + Linear (to predict end idx)\n",
        "        #############################################################################\n",
        "        directions = (2 if bidirectional else 1)\n",
        "        fc_dim = fc_dim if bidirectional else int(fc_dim/2)\n",
        "        self.start_linear = nn.Linear(fc_dim,1, bias=False)\n",
        "        self.end_lstmLayer = nn.LSTM(LSTM_input_size, LSTM_output_size, num_layers, bidirectional=bidirectional)\n",
        "        self.end_linear = nn.Linear(fc_dim, 1, bias=False)\n",
        "        self.start_softmax = nn.Softmax(dim=0)\n",
        "        self.end_softmax = nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, G, M):\n",
        "        start, end = None, None\n",
        "        #############################################################################\n",
        "        # G : query aware context word embeddings\n",
        "        # M : output of modeling layer\n",
        "        # returns :: `start` and `end` of size (T,) \n",
        "        #############################################################################\n",
        "        concat = torch.cat((G,M),1)\n",
        "        p_start = self.start_linear(concat)\n",
        "\n",
        "        m_2,_ = self.end_lstmLayer(M.view(len(M),1,-1))\n",
        "        m_2 = m_2.view(len(M),-1)\n",
        "        concat_2 = torch.cat((G,m_2),1)\n",
        "        p_end = self.end_linear(concat_2)\n",
        "\n",
        "        start = self.start_softmax(p_start).view(-1)\n",
        "        end = self.end_softmax(p_end).view(-1)\n",
        "\n",
        "        return start, end\n",
        "\n",
        " \n",
        "class BiDAF(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, char_embedding_dim,\n",
        "                 char_hidden_dim, char_size, vocab_size, bidirectional=True, phrase_LSTM_layers=1, modeling_LSTM_layers=2, dropout=0):\n",
        "        super(BiDAF, self).__init__()\n",
        "        #############################################################################\n",
        "        # Initialize all the modules created so far and link their inputs and \n",
        "        #       outputs properly:\n",
        "        #   LSTMEncoder\n",
        "        #   AttentionFlow\n",
        "        #   ModelingLayer\n",
        "        #   Output\n",
        "        #############################################################################\n",
        "        directions = (2 if bidirectional else 1)\n",
        "        self.LSTMEncoder = LSTMEncoder(embedding_dim, hidden_dim, char_embedding_dim,\n",
        "                 char_hidden_dim, char_size, vocab_size, phrase_LSTM_layers, \n",
        "                 bidirectional=bidirectional, dropout=dropout)\n",
        "        self.AttentionFlow = AttentionFlow(hidden_dim * 3 * directions)\n",
        "        self.ModelingLayer = ModelingLayer(hidden_dim * directions * 4, hidden_dim, modeling_LSTM_layers, dropout, \n",
        "        bidirectional)\n",
        "        self.Output = OutputLayer(hidden_dim*10, hidden_dim * directions, hidden_dim, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, context, context_chars, query, query_chars):\n",
        "        start, end = None, None\n",
        "        #############################################################################\n",
        "        # Given a tokenized index-mapped sentence and a character sequence as the arguments,\n",
        "        # find the corresponding scores for tags\n",
        "        # returns:: `start` and `end` of size (T,) \n",
        "        #   where, T is the number of words/tokens in context\n",
        "        #############################################################################\n",
        "        context_enc, query_enc = self.LSTMEncoder(context, context_chars, query, query_chars)\n",
        "        G = self.AttentionFlow(context_enc, query_enc)\n",
        "        M = self.ModelingLayer(G)\n",
        "        start, end = self.Output(G, M)\n",
        "\n",
        "        return start, end\n",
        "\n",
        "\n",
        "##### From Official SQUAD Evauation evaluation script version 2.0 #####\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, artcles and extra whitespace\"\"\"\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r'\\b(a|an|the)]b', re.UNICODE)\n",
        "        try:\n",
        "            return re.sub(regex, ' ', text)\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        try:\n",
        "            return ' '.join(text.split())\n",
        "        except:\n",
        "            return text\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        try:\n",
        "            return text.lower()\n",
        "        except:\n",
        "            return text\n",
        "    \n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n"
      ],
      "metadata": {
        "id": "I5K3Sr2EudKv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Eval"
      ],
      "metadata": {
        "id": "K2m_TIUn1Lia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Use above preprocessing, BiDAF model, and official squad eval method to train and eval model"
      ],
      "metadata": {
        "id": "FebsDoiO1RGq"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}